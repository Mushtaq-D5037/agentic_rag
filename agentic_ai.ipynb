{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e22c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from utils import (\n",
    "     prompts,\n",
    "     llm,    \n",
    ")\n",
    "from core import hybrid_search\n",
    "from utils.helpers import format_search_results, re_rank_docs\n",
    "from ddgs import DDGS \n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668d67c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting relevant documents from the vector database \n",
    "# Note: we are using qdrant vector db, which we already created for our UAE Legal RAG usecase\n",
    "query = \"what is the Managing director of Dubai Electricity Water Authority(DEWA)?\"\n",
    "retrivedDocs = hybrid_search.retreive_relevant_docs(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d8fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will make use of decision prompt to decide if we have relevant context or not\n",
    "# it returns 'Yes' if the context has an answer else it returns 'No' as answer\n",
    "decision_prompt = prompts.decision_prompt(query, retrivedDocs)\n",
    "has_answer, response_time = llm.generate_response( query=query, \n",
    "                                                   retrieved_results=retrivedDocs, \n",
    "                                                   prompt_template=decision_prompt,\n",
    "                                                   model_name=llm.GROQ_MODEL2\n",
    "                                                 )       \n",
    "\n",
    "print(f'has_answer: {has_answer}, response time (in millisec): {response_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the decision prompt result, we either generate response from context or search online using duckduckgo-search\n",
    "if has_answer == 'YES':\n",
    "    print('Context can answer the question, generating response...')\n",
    "    response_from_context = True\n",
    "else:\n",
    "    print('Context cannot answer the question, searching online for answer...')\n",
    "    response_from_context = False\n",
    "    response_from_ddgs = DDGS().text(query, max_results=5)\n",
    "    sorted_docs = re_rank_docs(response_from_ddgs, query)\n",
    "    formatted_results = format_search_results(sorted_docs)\n",
    "    \n",
    "retrieved_results = retrivedDocs if response_from_context else formatted_results\n",
    "\n",
    "# Common code for generating the response\n",
    "response, response_time = llm.generate_response(\n",
    "    query=query,\n",
    "    retrieved_results=retrieved_results,\n",
    "    prompt_template=prompts.response_generation(query, retrieved_results),\n",
    "    model_name=llm.GROQ_MODEL\n",
    ")\n",
    "\n",
    "print('Response:')\n",
    "display(Markdown(response))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".aiml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
